<?xml version='1.1' encoding='UTF-8'?>
<project>
  <actions/>
  <description></description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>admin_email</name>
          <defaultValue>CommonAreaUser</defaultValue>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
        <hudson.model.BooleanParameterDefinition>
          <name>CLEAR_DATABASE</name>
          <defaultValue>false</defaultValue>
        </hudson.model.BooleanParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>dictionary_etl_branch</name>
          <defaultValue>main</defaultValue>
          <trim>false</trim>
        </hudson.model.StringParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
  </properties>
  <scm class="hudson.plugins.git.GitSCM" plugin="git@5.5.2">
    <configVersion>2</configVersion>
    <userRemoteConfigs>
      <hudson.plugins.git.UserRemoteConfig>
        <url>https://github.com/hms-dbmi/picsure-dictionary-etl.git</url>
      </hudson.plugins.git.UserRemoteConfig>
    </userRemoteConfigs>
    <branches>
      <hudson.plugins.git.BranchSpec>
        <name>*/${dictionary_etl_branch}</name>
      </hudson.plugins.git.BranchSpec>
    </branches>
    <doGenerateSubmoduleConfigurations>false</doGenerateSubmoduleConfigurations>
    <submoduleCfg class="empty-list"/>
    <extensions/>
  </scm>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <jdk>(System)</jdk>
  <triggers/>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash
#fetch admin user long term token 
export EMAIL=$admin_email

export user_token=`docker run -i -v $MYSQL_CONFIG_DIR/.my.cnf:/root/.my.cnf --network=${MYSQL_NETWORK:-host} mysql mysql -N -e \
&quot;SELECT long_term_token FROM auth.user where email=&apos;${admin_email}&apos;; &quot;`

export hpds_uuid=`cat /usr/local/docker-config/httpd/picsureui_settings.json | grep picSureResourceId | cut -d &apos;:&apos; -f 2| sed &apos;s/[&quot;, ]*//g&apos;`
export base_url=`cat /usr/local/docker-config/httpd/httpd.env | grep VITE_ORIGIN | cut -d &apos;=&apos; -f 2`
curl -k --header &apos;Content-Type: application/json&apos; \
--header &quot;Authorization: Bearer ${user_token}&quot; \
 --location &quot;${base_url}/picsure/search/${hpds_uuid}&quot; \
--data &apos;{&quot;query&quot;:&quot;\\&quot;}&apos; | jq .results.phenotypes &gt; hpds_concept_list.json

echo &quot;Getting concept entries, with values and all ancestors, from hpds response&quot;
jq &apos;map({
    &quot;dataset_ref&quot;: .name | split(&quot;\\&quot;)[1],
    &quot;name&quot;:(.name | split(&quot;\\&quot;) | reverse[1]),
    &quot;display&quot;:(.name | split(&quot;\\&quot;) | reverse[1]),
    &quot;concept_type&quot;:
        (if (.categorical | not) then &quot;Continuous&quot; else &quot;Categorical&quot; end),
    &quot;concept_path&quot;:.name,
    &quot;parent_concept_path&quot;:
    (if ((.name | split(&quot;\\&quot;) | del(.[-2]) | join(&quot;\\&quot;)) != &quot;\\\\&quot;) then  (.name | split(&quot;\\&quot;) | del(.[-2]) | join(&quot;\\&quot;)) else &quot;&quot; end),
    &quot;values&quot;: ((if (.categorical | not) then [.min, .max] else .categoryValues end) | tostring)
}) | . + 
([.[].parent_concept_path  | split(&quot;\\&quot;) |
while (length &gt; 2;
    del(.[-2])
 )] | map(join(&quot;\\&quot;)) | unique
 | map ({
    &quot;dataset_ref&quot;: split(&quot;\\&quot;)[1],
    &quot;name&quot;:(split(&quot;\\&quot;) | reverse[1]),
    &quot;display&quot;:(split(&quot;\\&quot;) | reverse[1]),
    &quot;concept_type&quot;:&quot;Categorical&quot;,
    &quot;concept_path&quot;: .,
    &quot;parent_concept_path&quot;:
    (if ((split(&quot;\\&quot;) | del(.[-2]) | join(&quot;\\&quot;)) != &quot;\\&quot;) then  (split(&quot;\\&quot;) | del(.[-2]) | join(&quot;\\&quot;)) else &quot;&quot; end),
})) |  unique_by(.concept_path)&apos;  hpds_concept_list.json &gt; dictionary_concepts.json


echo &quot;building ideal ingest concepts.csv&quot;
jq -r &apos;([
  &quot;dataset_ref&quot;,
  &quot;name&quot;,
  &quot;display&quot;,
  &quot;concept_type&quot;,
  &quot;concept_path&quot;,
  &quot;parent_concept_path&quot;,
  &quot;values&quot;
]) as $cols | map(. as $row | $cols | 
map($row[.])) as $rows | $cols, $rows[] | @csv&apos; dictionary_concepts.json \
| sed &apos;s/\\/\\\\/g&apos; &gt; concepts.csv 


echo &quot;building ideal ingest datasets.csv&quot;
jq -r &apos;[.[].dataset_ref] | unique | .[] | [{
&quot;ref&quot;: .,
&quot;full_name&quot;: .,
&quot;abbreviation&quot;: &quot;&quot;,
&quot;description&quot;: &quot;&quot;}] | ([
&quot;ref&quot;,
&quot;full_name&quot;,
&quot;abbreviation&quot;,
&quot;description&quot;
] ) as $cols | map(. as $row | $cols | 
map($row[.])) as $rows | $cols, $rows[] | @csv&apos; dictionary_concepts.json &gt; datasets.csv 


</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
    <hudson.tasks.Shell>
      <command>#!/bin/bash
echo &quot;generation of ideal ingest files complete - moving to dictionary etl build&quot;
docker stop dictionaryetl || true
docker rm dictionaryetl || true
docker build -t hms-dbmi/dictionary-etl:hpds-api -t hms-dbmi/dictionary-etl:latest .

cp /usr/local/docker-config/dictionary/dictionary.env .env

docker stop dictionaryetl || true
docker rm dictionaryetl || true
docker run -d \
  --name dictionaryetl \
  --env-file .env \
  -p 8086:8086 \
  -v $DOCKER_CONFIG_DIR/hpds/:/opt/local/hpds/ \
  --network dictionary \
  --network picsure \
  hms-dbmi/dictionary-etl:latest

echo &quot;Waiting for DictionaryEtlApplication to start...&quot;
MAX_ATTEMPTS=6
SLEEP_INTERVAL=5
ATTEMPT=1

while ! docker logs dictionaryetl 2&gt;&amp;1 | grep -q &quot;Started DictionaryEtlApplication&quot;; do
    if [ &quot;$ATTEMPT&quot; -ge &quot;$MAX_ATTEMPTS&quot; ]; then
        echo &quot;DictionaryEtlApplication failed to start within $((MAX_ATTEMPTS * SLEEP_INTERVAL)) seconds.&quot;
        docker logs dictionaryetl
        docker stop dictionaryetl
        docker rm dictionaryetl
        exit 1
    fi
    echo &quot;Attempt $ATTEMPT: Application not started yet. Retrying in $SLEEP_INTERVAL seconds...&quot;
    sleep $SLEEP_INTERVAL
    ATTEMPT=$((ATTEMPT + 1))
done

echo &quot;DictionaryEtlApplication has started!&quot;

if [ &quot;$CLEAR_DATABASE&quot; ]; then
	echo &quot;Clearing database&quot;
	curl http://dictionaryetl:8086/load/clear
else
	echo &quot;Not clearing DB&quot;
fi
echo &quot;Adding datasets&quot;
curl -X PUT -T datasets.csv http://dictionaryetl:8086/api/dataset/csv

IFS=&apos;,&apos;      
while read ref fullname abv desc
do
if [[ $ref != *&apos;ref&apos;* ]]; then
	dataset=`echo ${ref} | tr -d &apos;&quot;&apos;`
    mkdir $dataset
	echo &quot;Fetching concepts for $dataset&quot;
	head -n 1 concepts.csv &gt; ${dataset}/concepts.csv
    grep -h -e &quot;^\&quot;*$dataset&quot; concepts.csv &gt;&gt; ${dataset}/concepts.csv
	encoded_dataset=$(printf &quot;$dataset&quot; | sed &apos;s/ /%20/g&apos;)
    echo &quot;Building concepts for $dataset&quot;
    if [[  $(wc -l &lt; ${dataset}/concepts.csv) -gt 50000 ]];
    then
    	{
          echo &apos;large dataset - splitting files to prevent oom&apos;
          i=1
          line1=2
          while [[ $line1 -lt $(wc -l &lt; ${dataset}/concepts.csv) ]];
              do
                  line2=$(($line1+50000))
                   
                  if [[ $line2 -gt $(wc -l &lt; ${dataset}/concepts.csv) ]]; then
                      line2=`wc -l &lt; ${dataset}/concepts.csv`
                  fi
                  echo &quot;started $dataset line1 $line1 through line2 $line2&quot;
                 
                    head -n 1 ${dataset}/concepts.csv &gt; ${dataset}/concepts_$i.csv
                    sed -n &quot;${line1},${line2}p&quot; ${dataset}/concepts.csv &gt;&gt; ${dataset}/concepts_$i.csv
                    curl --request PUT --header &apos;Content-Type: text/plain&apos; \
                        --data-binary @${dataset}/concepts_$i.csv \
                        &quot;http://dictionaryetl:8086/api/concept/csv?datasetRef=${encoded_dataset}&quot;
                        
                    echo &quot;completed $dataset line1 $line1 through line2 $line2&quot;
                  
                  i=$(($i+1))
                  line1=$line2
              done
          } &amp;
    else
	{
      echo started $dataset ingest
      curl --request PUT --header &apos;Content-Type: text/plain&apos; \
        --data-binary @${dataset}/concepts.csv \
        &quot;http://dictionaryetl:8086/api/concept/csv?datasetRef=${encoded_dataset}&quot;
      echo completed $dataset ingest
    } &amp;
    fi
else
	echo &quot;skipping header line&quot;
fi
done &lt; datasets.csv
wait;
echo Completed all dataset updates, refreshing base continuous/categorical facets
      curl --request PUT \
        &quot;http://dictionaryetl:8086/api/facet/refresh/data_type&quot;

docker exec -i dictionary-db psql dictionary picsure -c &apos;UPDATE dict.update_info SET last_updated = NOW();&apos;
cp datasets.csv /usr/local/docker-config/dictionary/
cp concepts.csv /usr/local/docker-config/dictionary/</command>
      <configuredLocalRules/>
    </hudson.tasks.Shell>
  </builders>
  <publishers>
    <hudson.tasks.ArtifactArchiver>
      <artifacts>datasets.csv,concepts.csv</artifacts>
      <allowEmptyArchive>false</allowEmptyArchive>
      <onlyIfSuccessful>true</onlyIfSuccessful>
      <fingerprint>false</fingerprint>
      <defaultExcludes>true</defaultExcludes>
      <caseSensitive>true</caseSensitive>
      <followSymlinks>false</followSymlinks>
    </hudson.tasks.ArtifactArchiver>
  </publishers>
  <buildWrappers/>
</project>